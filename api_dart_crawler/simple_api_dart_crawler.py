# dart_api_collector_v3.py
# ì‹¤í–‰ ì „: pip install requests pandas tqdm
# config.py ë‚´ë¶€ì— api_key ë³€ìˆ˜ë¡œ OpenDART API í‚¤ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: api_key = "ë³¸ì¸_OpenDART_API_KEY"

import os
import time
import requests
import json
import pandas as pd
from tqdm import tqdm
import zipfile
import io
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import config

# ----------------------- ì„¤ì • ì˜ì—­ -----------------------
API_KEY = config.api_key
BASE_SAVE_DIR = r"C:\Users\User\OneDrive\ë¬¸ì„œ\ê°œë°œ100ì œ\total_finance_crawling\naver_news_crawler\api_dart_crawler"
PROCESSED_LOG_FILE = os.path.join(BASE_SAVE_DIR, "processed_companies.log")
os.makedirs(BASE_SAVE_DIR, exist_ok=True)

# â­ ë°°ì¹˜ ì‹¤í–‰ ì„¤ì • â­
BATCH_SIZE = 400
START_INDEX = 0
END_INDEX = None

# ë””ë²„ê¹… ëª¨ë“œ
DEBUG_MODE = True

# ê¸°ê°„ ì„¤ì •
current_year = datetime.now().year
start_year = current_year - 10
end_year = current_year

print(f"ìˆ˜ì§‘ ê¸°ê°„: {start_year}ë…„ ~ {end_year}ë…„")

# OpenDART API ì—”ë“œí¬ì¸íŠ¸
API_CORP_CODE = "https://opendart.fss.or.kr/api/corpCode.xml"
API_FNLTT_ALL = "https://opendart.fss.or.kr/api/fnlttSinglAcntAll.json"

# ë³´ê³ ì„œ ì½”ë“œ ìš°ì„ ìˆœìœ„
REPRT_PRIORITY = [('11013', '1ë¶„ê¸°ë³´ê³ ì„œ' ), ('11012', 'ë°˜ê¸°ë³´ê³ ì„œ'), ('11014', '3ë¶„ê¸°ë³´ê³ ì„œ'), ('11011', 'ì‚¬ì—…ë³´ê³ ì„œ')]
REPORT_DATE_MAP = {'11013': '03-31', '11012': '06-30', '11014': '09-30', '11011': '12-31'}

# ê³„ì • ë° í‚¤ì›Œë“œ ë§¤í•‘ (ê¸°ì¡´ê³¼ ë™ì¼)
account_map = {
    'ìì‚°ì´ê³„': ['ifrs-full_Assets', 'dart_TotalAssets', 'ifrs_Assets', 'Assets'],
    'ë¶€ì±„ì´ê³„': ['ifrs-full_Liabilities', 'dart_TotalLiabilities', 'ifrs_Liabilities', 'Liabilities'],
    'ìë³¸ì´ê³„': ['ifrs-full_Equity', 'dart_TotalEquity', 'ifrs_Equity', 'Equity'],
    'ë§¤ì¶œì•¡': ['ifrs-full_Revenue', 'dart_Revenue', 'Revenue'],
    'ë‹¹ê¸°ìˆœì´ìµ': ['ifrs-full_ProfitLoss', 'dart_ProfitLoss', 'ProfitLoss']
}
keyword_map = {
    'ìì‚°ì´ê³„': ['ìì‚°ì´ê³„', 'ì´ìì‚°'], 'ë¶€ì±„ì´ê³„': ['ë¶€ì±„ì´ê³„', 'ì´ë¶€ì±„'], 'ìë³¸ì´ê³„': ['ìë³¸ì´ê³„', 'ì´ìë³¸'],
    'ë§¤ì¶œì•¡': ['ë§¤ì¶œì•¡', 'ì˜ì—…ìˆ˜ìµ'], 'ë‹¹ê¸°ìˆœì´ìµ': ['ë‹¹ê¸°ìˆœì´ìµ', 'ìˆœì´ìµ']
}

# ----------------------- ìœ í‹¸ í•¨ìˆ˜ë“¤ -----------------------
def debug_log(message, level="INFO"):
    if DEBUG_MODE:
        print(f"    [{level}] {datetime.now().strftime('%H:%M:%S')} - {message}")

def safe_parse_num(x):
    if x is None: return None
    s = str(x).strip().replace(',', '')
    if not s or s.lower() in ['nan', 'none', 'null', '-']: return None
    if s.startswith('(') and s.endswith(')'): s = '-' + s[1:-1]
    try: return float(s)
    except (ValueError, TypeError): return None

# â­ [ìˆ˜ì •ë¨] ìƒì¥íì§€ ê¸°ì—… í•„í„°ë§ ë¡œì§ ì¶”ê°€
def fetch_all_listed_companies(api_key, timeout=60):
    params = {'crtfc_key': api_key}
    try:
        print("ğŸŒ OpenDARTì—ì„œ ì „ì²´ ê¸°ì—… ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        r = requests.get(API_CORP_CODE, params=params, timeout=timeout)
        r.raise_for_status()
        
        with zipfile.ZipFile(io.BytesIO(r.content)) as z:
            xml_filename = z.namelist()[0]
            with z.open(xml_filename) as f:
                root = ET.fromstring(f.read())
        
        listed_companies = {}
        total_corps = 0
        delisted_corps = 0
        # ìƒì¥ íì§€ í•„í„°ë§ ê¸°ì¤€ ë‚ ì§œ (í˜„ì¬ë¡œë¶€í„° ì•½ 2ë…„ ì „)
        delist_cutoff_date = datetime.now() - timedelta(days=365 * 2)

        for corp in root.findall('list'):
            total_corps += 1
            stock_code_node = corp.find('stock_code')
            # ì¢…ëª© ì½”ë“œê°€ ì—†ëŠ” ê¸°ì—…(ë¹„ìƒì¥ ë“±)ì€ ì œì™¸
            if stock_code_node is None or not stock_code_node.text.strip():
                continue

            # ìƒì¥ íì§€ì¼(modify_date) í™•ì¸
            modify_date_str = corp.find('modify_date').text
            if modify_date_str:
                try:
                    modify_date = datetime.strptime(modify_date_str, '%Y%m%d')
                    # ìƒì¥ íì§€ì¼ì´ 2ë…„ë³´ë‹¤ ë” ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ê±´ë„ˆëœ€
                    if modify_date < delist_cutoff_date:
                        delisted_corps += 1
                        continue
                except ValueError:
                    pass # ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš° ë¬´ì‹œ

            stock_code = stock_code_node.text.strip()
            if len(stock_code) == 6 and stock_code.isdigit():
                listed_companies[stock_code] = {
                    'corp_code': corp.find('corp_code').text,
                    'corp_name': corp.find('corp_name').text,
                    'stock_code': stock_code
                }
        
        print(f"âœ… ì „ì²´ ê¸°ì—… {total_corps}ê°œ ì¤‘, ì˜¤ë˜ì „ ìƒì¥íì§€ëœ ê¸°ì—… {delisted_corps}ê°œë¥¼ ì œì™¸í•˜ê³  ìœ íš¨í•œ {len(listed_companies)}ê°œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
        return listed_companies
        
    except Exception as e:
        print(f"âŒ ìƒì¥ê¸°ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return {}

def fetch_fnltt_all(api_key, corp_code, bsns_year, reprt_code, fs_div='CFS', timeout=30):
    params = {'crtfc_key': api_key, 'corp_code': corp_code, 'bsns_year': str(bsns_year), 'reprt_code': reprt_code, 'fs_div': fs_div}
    debug_log(f"API í˜¸ì¶œ: {corp_code}, {bsns_year}, {reprt_code}, {fs_div}")
    try:
        r = requests.get(API_FNLTT_ALL, params=params, timeout=timeout)
        r.raise_for_status()
        result = r.json()
        if result.get('status') != '000':
            debug_log(f"âš ï¸ API ì‘ë‹µ: status={result.get('status')}, message={result.get('message', '')}")
        return result
    except Exception as e:
        debug_log(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}", "ERROR")
        return {'status': '999', 'message': f'request_failed:{e}'}

def extract_values_from_list(items_list, account_tags):
    if not items_list: return None
    for tag in account_tags:
        for item in items_list:
            if item.get('account_id', '').strip().lower() == tag.lower():
                return safe_parse_num(item.get('thstrm_amount'))
    return None

def find_by_account_name_keywords(items_list, keywords):
    if not items_list: return None
    for keyword in keywords:
        for item in items_list:
            if keyword in item.get('account_nm', ''):
                return safe_parse_num(item.get('thstrm_amount'))
    return None

# ----------------------- í•µì‹¬ ë¡œì§ -----------------------

# â­ [ìˆ˜ì •ë¨] ì¡°ê¸° ì¤‘ë‹¨ ë¡œì§ ì¶”ê°€
def collect_company_data(api_key, corp_code, corp_name, stock_code):
    rows = []
    consecutive_failures = 0 # ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜
    FAILURE_THRESHOLD = 3    # ì—°ì† 3íšŒ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ê¸°ì—… ê±´ë„ˆë›°ê¸°

    for year in range(end_year, start_year - 1, -1): # ìµœì‹  ì—°ë„ë¶€í„° íƒìƒ‰
        collected_reports_for_year = set()
        for reprt_code, report_name in REPRT_PRIORITY:
            if reprt_code in collected_reports_for_year: continue
            
            # ì¡°ê¸° ì¤‘ë‹¨ ì²´í¬
            if consecutive_failures >= FAILURE_THRESHOLD:
                print(f"    âš ï¸ ì—°ì† {FAILURE_THRESHOLD}íšŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨. ì´ ê¸°ì—…ì˜ ë‚¨ì€ ê¸°ê°„ ì¡°íšŒë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                if not rows: return None
                else: return pd.DataFrame(rows)

            time.sleep(0.2)
            
            result = fetch_fnltt_all(api_key, corp_code, year, reprt_code, fs_div='CFS')
            if result.get('status') != '000' or not result.get('list'):
                debug_log(f"CFS ë°ì´í„° ì—†ìŒ ({year} {report_name}), OFS ì‹œë„...")
                result = fetch_fnltt_all(api_key, corp_code, year, reprt_code, fs_div='OFS')

            if result.get('status') == '000' and result.get('list'):
                consecutive_failures = 0 # ì„±ê³µ ì‹œ ì´ˆê¸°í™”
                items = result['list']
                debug_log(f"âœ… ë°ì´í„° ë°œê²¬: {year}ë…„ {report_name} ({len(items)}ê°œ ê³„ì •)")
                
                row = {}
                found_any = False
                for label, tags in account_map.items():
                    value = extract_values_from_list(items, tags)
                    if value is None: value = find_by_account_name_keywords(items, keyword_map.get(label, []))
                    row[label] = value
                    if value is not None: found_any = True
                
                if found_any:
                    row.update({'bsns_year': year, 'reprt_code': reprt_code, 'report_name': report_name,
                                'report_date': f"{year}-{REPORT_DATE_MAP[reprt_code]}",
                                'ê¸°ì—…ëª…': corp_name, 'ì¢…ëª©ì½”ë“œ': stock_code})
                    rows.append(row)
                    collected_reports_for_year.add(reprt_code)
            else:
                debug_log(f"ë°ì´í„° ì—†ìŒ: {year}ë…„ {report_name}")
                consecutive_failures += 1

    if not rows:
        print(f"    âŒ ì „ì²´ ê¸°ê°„ì—ì„œ ìœ ì˜ë¯¸í•œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return None
    
    print(f"    âœ… ì´ {len(rows)}ê°œ ë¶„ê¸°/ë°˜ê¸°/ì—°ê°„ ë³´ê³ ì„œ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
    return pd.DataFrame(rows)

def save_company_data(corp_name, stock_code, df):
    if df is None or df.empty: return False
    safe_name = "".join(c for c in corp_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
    file_name = f"dart_financial_{safe_name}_{stock_code}.csv"
    save_path = os.path.join(BASE_SAVE_DIR, file_name)
    try:
        df.to_csv(save_path, index=False, encoding="utf-8-sig")
        print(f"    ğŸ’¾ ì €ì¥ ì™„ë£Œ: {file_name}")
        return True
    except Exception as e:
        print(f"    âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def load_processed_companies():
    if not os.path.exists(PROCESSED_LOG_FILE): return set()
    with open(PROCESSED_LOG_FILE, 'r', encoding='utf-8') as f:
        processed = {line.strip() for line in f}
    print(f"ğŸ“– ì´ì „ì— ì²˜ë¦¬ëœ ê¸°ì—… {len(processed)}ê°œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    return processed

def log_processed_company(stock_code):
    with open(PROCESSED_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{stock_code}\n")

# ----------------------- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ -----------------------
def main():
    print("ğŸš€ OpenDART ì¬ë¬´ë°ì´í„° ìˆ˜ì§‘ê¸° v3 (ìƒì¥íì§€ ê¸°ì—… í•„í„°ë§ ë° ì¡°ê¸° ì¤‘ë‹¨ ê¸°ëŠ¥ ì¶”ê°€)")
    print("=" * 60)
    
    listed_companies_dict = fetch_all_listed_companies(API_KEY)
    if not listed_companies_dict: return

    processed_stock_codes = load_processed_companies()
    companies_to_process = [(code, info) for code, info in listed_companies_dict.items() if code not in processed_stock_codes]
    
    start = START_INDEX
    end = END_INDEX if END_INDEX is not None else len(companies_to_process)
    batch_companies = companies_to_process[start:end]
    if BATCH_SIZE > 0: batch_companies = batch_companies[:BATCH_SIZE]

    print(f"\nğŸ“Š ë°°ì¹˜ ì •ë³´: ì „ì²´ {len(listed_companies_dict):,}ê°œ ì¤‘ ë¯¸ì²˜ë¦¬ {len(companies_to_process):,}ê°œ / ì´ë²ˆ ì„¸ì…˜ ì²˜ë¦¬ ëŒ€ìƒ: {len(batch_companies)}ê°œ")
    print("-" * 60)

    if not batch_companies:
        print("ğŸ‰ ëª¨ë“  ê¸°ì—…ì˜ ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return

    success_count = 0
    for stock_code, corp_info in tqdm(batch_companies, desc="ê¸°ì—… ì²˜ë¦¬ ì¤‘"):
        print(f"\n\n[ì²˜ë¦¬ ì‹œì‘] {corp_info['corp_name']}({stock_code})")
        try:
            df = collect_company_data(API_KEY, corp_info['corp_code'], corp_info['corp_name'], stock_code)
            if df is not None and not df.empty:
                if save_company_data(corp_info['corp_name'], stock_code, df):
                    success_count += 1
            log_processed_company(stock_code)
        except Exception as e:
            print(f"  âŒ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜ˆì™¸ ë°œìƒ: {e}")
            log_processed_company(stock_code)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“‹ ì´ë²ˆ ì„¸ì…˜ì—ì„œ {success_count}ê°œ ê¸°ì—…ì˜ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
