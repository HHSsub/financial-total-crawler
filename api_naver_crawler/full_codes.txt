import requests
import pandas as pd
import os
import re
import time
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
from typing import Dict, List, Set, Tuple, Optional
from bs4 import BeautifulSoup
from pykrx import stock
import config

# 로깅 설정
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KRXDataCollector:
    """PyKRX를 사용한 상장종목 데이터 수집 클래스"""
    
    def __init__(self):
        pass
        
    def get_all_listed_companies(self) -> pd.DataFrame:
        """전체 상장기업 목록을 가져옴 (PyKRX 사용)"""
        logger.info("PyKRX를 통해 전체 상장기업 목록 조회 중...")
        
        try:
            # 최근 영업일 기준으로 전체 상장기업 조회
            all_companies = []
            
            # KOSPI 상장기업
            logger.info("KOSPI 상장기업 조회 중...")
            kospi_tickers = stock.get_market_ticker_list(market="KOSPI")
            for ticker in tqdm(kospi_tickers, desc="KOSPI 기업 정보 수집"):
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_companies.append({
                        '종목명': name,
                        '종목코드': ticker,
                        '시장구분': 'KOSPI'
                    })
                    time.sleep(0.01)  # API 호출 제한 방지
                except Exception as e:
                    logger.warning(f"KOSPI {ticker} 처리 오류: {e}")
                    continue
            
            # KOSDAQ 상장기업
            logger.info("KOSDAQ 상장기업 조회 중...")
            kosdaq_tickers = stock.get_market_ticker_list(market="KOSDAQ")
            for ticker in tqdm(kosdaq_tickers, desc="KOSDAQ 기업 정보 수집"):
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_companies.append({
                        '종목명': name,
                        '종목코드': ticker,
                        '시장구분': 'KOSDAQ'
                    })
                    time.sleep(0.01)  # API 호출 제한 방지
                except Exception as e:
                    logger.warning(f"KOSDAQ {ticker} 처리 오류: {e}")
                    continue
            
            # KONEX 상장기업 (추가)
            logger.info("KONEX 상장기업 조회 중...")
            try:
                konex_tickers = stock.get_market_ticker_list(market="KONEX")
                for ticker in tqdm(konex_tickers, desc="KONEX 기업 정보 수집"):
                    try:
                        name = stock.get_market_ticker_name(ticker)
                        all_companies.append({
                            '종목명': name,
                            '종목코드': ticker,
                            '시장구분': 'KONEX'
                        })
                        time.sleep(0.01)
                    except Exception as e:
                        logger.warning(f"KONEX {ticker} 처리 오류: {e}")
                        continue
            except Exception as e:
                logger.warning(f"KONEX 조회 오류: {e}")
            
            companies_df = pd.DataFrame(all_companies)
            logger.info(f"이 {len(companies_df)}개 상장기업 정보 수집 완료")
            return companies_df
            
        except Exception as e:
            logger.error(f"PyKRX 조회 중 오류: {e}")
            return self._get_fallback_companies()
    
    def _get_fallback_companies(self) -> pd.DataFrame:
        """PyKRX 실패시 대체 방법으로 주요 상장기업 목록 반환"""
        logger.info("대체 방법으로 주요 상장기업 목록 생성 중...")
        
        # 주요 상장기업들 (약 100개 정도 포함)
        major_companies = [
            {'종목명': '삼성전자', '종목코드': '005930', '시장구분': 'KOSPI'},
            {'종목명': 'SK하이닉스', '종목코드': '000660', '시장구분': 'KOSPI'},
            {'종목명': '삼성바이오로직스', '종목코드': '207940', '시장구분': 'KOSPI'},
            {'종목명': 'LG에너지솔루션', '종목코드': '373220', '시장구분': 'KOSPI'},
            {'종목명': '현대자동차', '종목코드': '005380', '시장구분': 'KOSPI'},
            {'종목명': 'KB금융', '종목코드': '105560', '시장구분': 'KOSPI'},
            {'종목명': '기아', '종목코드': '000270', '시장구분': 'KOSPI'},
            {'종목명': '셀트리온', '종목코드': '068270', '시장구분': 'KOSPI'},
            {'종목명': 'NAVER', '종목코드': '035420', '시장구분': 'KOSPI'},
            {'종목명': '카카오', '종목코드': '035720', '시장구분': 'KOSPI'},
            {'종목명': '삼성물산', '종목코드': '028260', '시장구분': 'KOSPI'},
            {'종목명': '현대모비스', '종목코드': '012330', '시장구분': 'KOSPI'},
            {'종목명': 'POSCO홀딩스', '종목코드': '005490', '시장구분': 'KOSPI'},
            {'종목명': '한국전력', '종목코드': '015760', '시장구분': 'KOSPI'},
            {'종목명': 'LG화학', '종목코드': '051910', '시장구분': 'KOSPI'},
            {'종목명': '크래프톤', '종목코드': '259960', '시장구분': 'KOSPI'},
            {'종목명': '카카오뱅크', '종목코드': '323410', '시장구분': 'KOSPI'},
            {'종목명': '삼성SDS', '종목코드': '018260', '시장구분': 'KOSPI'},
            {'종목명': 'KT', '종목코드': '030200', '시장구분': 'KOSPI'},
            {'종목명': 'LG전자', '종목코드': '066570', '시장구분': 'KOSPI'},
            {'종목명': 'SK텔레콤', '종목코드': '017670', '시장구분': 'KOSPI'},
            {'종목명': '하이브', '종목코드': '352820', '시장구분': 'KOSPI'},
            {'종목명': '대한항공', '종목코드': '003490', '시장구분': 'KOSPI'},
            {'종목명': '카카오페이', '종목코드': '377300', '시장구분': 'KOSPI'},
            {'종목명': '아모레퍼시픽', '종목코드': '090430', '시장구분': 'KOSPI'},
            {'종목명': 'NH투자증권', '종목코드': '005940', '시장구분': 'KOSPI'},
            {'종목명': 'LS Electric', '종목코드': '010120', '시장구분': 'KOSPI'},
            {'종목명': '코웨이', '종목코드': '021240', '시장구분': 'KOSPI'},
            {'종목명': 'SK바이오팜', '종목코드': '326030', '시장구분': 'KOSPI'},
            {'종목명': '한화솔루션', '종목코드': '009830', '시장구분': 'KOSPI'},
            {'종목명': 'LG유플러스', '종목코드': '032640', '시장구분': 'KOSPI'},
            {'종목명': '키움증권', '종목코드': '039490', '시장구분': 'KOSPI'},
            {'종목명': 'SK이노베이션', '종목코드': '096770', '시장구분': 'KOSPI'},
            {'종목명': '포스코인터내셔날', '종목코드': '047050', '시장구분': 'KOSPI'},
            {'종목명': 'LG디스플레이', '종목코드': '034220', '시장구분': 'KOSPI'},
            {'종목명': '한국조선해양', '종목코드': '009540', '시장구분': 'KOSPI'},
            {'종목명': '삼성화재', '종목코드': '000810', '시장구분': 'KOSPI'},
            {'종목명': '신한지주', '종목코드': '055550', '시장구분': 'KOSPI'},
            {'종목명': '하나금융지주', '종목코드': '086790', '시장구분': 'KOSPI'},
            {'종목명': 'SK', '종목코드': '034730', '시장구분': 'KOSPI'},
            # KOSDAQ 주요 기업들
            {'종목명': 'HLB', '종목코드': '028300', '시장구분': 'KOSDAQ'},
            {'종목명': '에코프로', '종목코드': '086520', '시장구분': 'KOSDAQ'},
            {'종목명': '에코프로비엠', '종목코드': '247540', '시장구분': 'KOSDAQ'},
            {'종목명': '펄어비스', '종목코드': '263750', '시장구분': 'KOSDAQ'},
            {'종목명': '엔씨소프트', '종목코드': '036570', '시장구분': 'KOSDAQ'},
            {'종목명': '위메이드', '종목코드': '112040', '시장구분': 'KOSDAQ'},
            {'종목명': '컴투스', '종목코드': '078340', '시장구분': 'KOSDAQ'},
            {'종목명': '네오위즈', '종목코드': '095660', '시장구분': 'KOSDAQ'},
            {'종목명': '알테오젠', '종목코드': '196170', '시장구분': 'KOSDAQ'},
            {'종목명': '메디톡스', '종목코드': '086900', '시장구분': 'KOSDAQ'},
        ]
        
        return pd.DataFrame(major_companies)

class NewsAnalyzer:
    def __init__(self):
        self.CLIENT_ID = config.client_id
        self.CLIENT_SECRET = config.client_secret
        self.BASE_URL = 'https://openapi.naver.com/v1/search/news.json'
        
        # KRX 데이터 수집기 초기화
        self.krx_collector = KRXDataCollector()
        
        # 설정값
        self.START_DATE = '2015-01-01'
        self.END_DATE = '2024-12-31'
        self.MAX_ARTICLES_PER_COMPANY = 500  # 기업당 최대 기사 수 조정
        self.MAX_VARIATIONS_PER_COMPANY = 3   # 기업당 검색 변형 수 제한
        self.REQUEST_DELAY = 0.1
        
        # 데이터 자산 키워드를 카테고리별로 분류
        self.data_asset_keywords = {
            '기본': [
                '데이터', '데이터자산', '빅데이터', 'AI', '인공지능', '머신러닝',
                '딥러닝', '알고리즘', '데이터분석', 'DB', '데이터베이스'
            ],
            '인프라': [
                '클라우드', '데이터센터', '데이터 플랫폼', '데이터 레이크',
                '데이터 웨어하우스', 'ETL', 'API', '분산저장', '엣지컴퓨팅',
                '슈퍼컴퓨터', 'GPU', 'NPU'
            ],
            '활용분야': [
                '자연언어처리', '컴퓨터비전', '추천시스템', '예측모델링',
                '헬스 데이터', '유전체 데이터', '바이오데이터',
                'IoT 데이터', '자율주행 데이터',
                'CRM 데이터', 'ERP 데이터', '로그데이터', '사용자 행동 데이터'
            ],
            '무형연계자산': [
                '소프트웨어 자산', '지식재산권', '디지털 트윈', '메타버스 자산'
            ],
            '신흥영역': [
                '생성형AI', 'LLM', '합성데이터', '데이터 라벨링',
                '데이터 거버넌스', '데이터 품질', '데이터 윤리', '데이터 보안', '데이터 프라이버시'
            ]
        }
        
        # 전체 키워드 리스트
        self.all_keywords = []
        for keywords in self.data_asset_keywords.values():
            self.all_keywords.extend(keywords)
        
        # 수집된 기사 중복 방지
        self.collected_articles: Set[str] = set()
        
        # 상장기업 정보 저장
        self.companies_df: Optional[pd.DataFrame] = None
        
    def load_companies(self):
        """상장기업 목록 로드"""
        if self.companies_df is None:
            self.companies_df = self.krx_collector.get_all_listed_companies()
            
    def generate_company_search_variations(self, company_row) -> List[str]:
        """기업 검색 변형 생성 (개선된 버전)"""
        variations = set()
        
        # 기본 정보에서 변형 생성
        company_name = company_row.get('종목명', '')
        stock_code = company_row.get('종목코드', '')
        
        if not company_name:
            return []
        
        # 기본 이름 추가
        variations.add(company_name)
        
        # 주식회사, (주) 제거/추가 변형
        clean_name = re.sub(r'주식회사$|㈜|주식회사|（주）|\(주\)', '', company_name).strip()
        if clean_name and len(clean_name) >= 2:
            variations.add(clean_name)
            variations.add(clean_name + '주식회사')
            variations.add(f'(주){clean_name}')
            variations.add(f'㈜{clean_name}')
        
        # 공백 처리
        if ' ' in company_name:
            variations.add(company_name.replace(' ', ''))
        if '-' in company_name:
            variations.add(company_name.replace('-', ' '))
        
        # 영문 대소문자 변형 (영문 포함시에만)
        if re.search(r'[a-zA-Z]', company_name):
            variations.add(company_name.upper())
            variations.add(company_name.lower())
            variations.add(company_name.title())
        
        # 종목코드 추가 (6자리인 경우만)
        if stock_code and len(stock_code) == 6:
            variations.add(stock_code)
        
        # 너무 짧은 변형들 제거 (2글자 미만)
        variations = {v for v in variations if len(v) >= 2}
        
        # 상위 N개만 반환 (API 호출 최적화)
        return list(variations)[:self.MAX_VARIATIONS_PER_COMPANY]
    
    def get_full_article_content(self, url: str) -> str:
        """기사 전문 가져오기 (안정적인 버전)"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            time.sleep(0.2)  # 서버 부하 방지
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 네이버 뉴스 기사 본문 추출 선택자들
                content_selectors = [
                    '#dic_area',           # 네이버 뉴스 메인
                    '.news_end',           # 일반적인 뉴스 사이트
                    '.article_body',       # 기사 본문
                    '.article-body',
                    '.content',
                    '.article_content',
                    'article',
                    '.view_txt',           # 일부 뉴스사
                    '.article-wrap',       # 추가 선택자
                    '.news_view'           # 추가 선택자
                ]
                
                article_content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        # 광고, 스크립트 등 불필요한 요소 제거
                        for unwanted in content_elem.select('script, style, .ad, .advertisement, .social, .share, .comment, .footer'):
                            unwanted.decompose()
                        
                        article_content = content_elem.get_text(strip=True)
                        if len(article_content) > 200:  # 충분한 길이의 본문인 경우
                            break
                
                # 본문이 없으면 전체 텍스트에서 추출
                if not article_content or len(article_content) < 200:
                    # 불필요한 요소들 제거 후 텍스트 추출
                    for unwanted in soup.select('script, style, nav, header, footer, .ad, .advertisement'):
                        unwanted.decompose()
                    article_content = soup.get_text()
                
                # 텍스트 정리
                article_content = re.sub(r'\s+', ' ', article_content).strip()
                return article_content[:5000]  # 최대 5000자
                
        except Exception as e:
            logger.warning(f"기사 본문 추출 실패 {url}: {e}")
        
        return ""
    
    def search_news_for_query(self, query: str, start_date: str, end_date: str, max_articles: int = 500) -> List[Dict]:
        """특정 쿼리로 뉴스 검색 (안정화된 버전)"""
        headers = {
            "X-Naver-Client-Id": self.CLIENT_ID,
            "X-Naver-Client-Secret": self.CLIENT_SECRET
        }
        
        articles = []
        display = 100
        
        for start in range(1, max_articles + 1, display):
            if start > 1000:  # 네이버 API 제한
                break
            
            current_display = min(display, 1000 - start + 1, max_articles - len(articles))
            if current_display <= 0:
                break
            
            params = {
                'query': query,
                'display': current_display,
                'start': start,
                'sort': 'sim'
            }
            
            try:
                time.sleep(self.REQUEST_DELAY)
                response = requests.get(self.BASE_URL, headers=headers, params=params, timeout=10)
                
                if response.status_code != 200:
                    logger.warning(f"[{query}] API 오류 {response.status_code}")
                    if response.status_code == 429:  # Too Many Requests
                        time.sleep(1)  # 추가 대기
                        continue
                    break
                
                data = response.json()
                items = data.get('items', [])
                
                if not items:
                    break
                
                for item in items:
                    try:
                        # 날짜 파싱
                        pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')
                        pub_str = pub_date.strftime('%Y-%m-%d')
                        
                        if not (start_date <= pub_str <= end_date):
                            continue
                        
                        # 중복 체크
                        if item['link'] in self.collected_articles:
                            continue
                        
                        article = {
                            'pubDate': pub_str,
                            'title': self._clean_html_tags(item['title']),
                            'description': self._clean_html_tags(item['description']),
                            'link': item['link'],
                            'original_query': query,
                            'full_content': ''  # 나중에 채움
                        }
                        
                        articles.append(article)
                        self.collected_articles.add(item['link'])
                        
                        if len(articles) >= max_articles:
                            return articles
                            
                    except Exception as e:
                        logger.error(f"기사 처리 오류: {e}")
                        continue
                
                if len(items) < current_display:
                    break
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"API 요청 오류: {e}")
                time.sleep(2)  # 네트워크 오류시 대기
                break
            except Exception as e:
                logger.error(f"예상치 못한 오류: {e}")
                break
        
        return articles
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML 태그 및 특수문자 제거 (개선된 버전)"""
        if not text:
            return ""
        
        # HTML 태그 제거
        clean = re.sub('<.*?>', '', text)
        
        # HTML 엔티티 디코딩
        html_entities = {
            '&lt;': '<', '&gt;': '>', '&amp;': '&', '&quot;': '"',
            '&#39;': "'", '&nbsp;': ' ', '&apos;': "'", 
            '&ldquo;': '"', '&rdquo;': '"', '&lsquo;': "'", '&rsquo;': "'",
            '&hellip;': '...', '&mdash;': '—', '&ndash;': '–'
        }
        
        for entity, replacement in html_entities.items():
            clean = clean.replace(entity, replacement)
        
        # 연속된 공백 제거
        clean = re.sub(r'\s+', ' ', clean)
        
        return clean.strip()
    
    def match_keywords_by_category(self, text: str) -> Dict[str, List[str]]:
        """카테고리별 키워드 매칭 (정확도 개선)"""
        text_lower = text.lower()
        matched_by_category = {}
        
        for category, keywords in self.data_asset_keywords.items():
            matched_keywords = []
            for keyword in keywords:
                keyword_lower = keyword.lower()
                
                # 단어 경계를 고려한 매칭 또는 부분 매칭
                if (re.search(r'\b' + re.escape(keyword_lower) + r'\b', text_lower) or 
                    (len(keyword_lower) >= 3 and keyword_lower in text_lower)):
                    matched_keywords.append(keyword)
            
            matched_by_category[category] = matched_keywords
        
        return matched_by_category
    
    def match_company(self, text: str, company_variations: List[str], company_info: Dict) -> bool:
        """기업명 매칭 확인 (정확도 개선)"""
        text_lower = text.lower()
        
        for variation in company_variations:
            var_lower = variation.lower()
            
            # 길이별 매칭 전략
            if len(var_lower) <= 2:
                # 짧은 이름은 완전 일치만
                if var_lower == text_lower:
                    return True
            elif len(var_lower) <= 4:
                # 중간 길이는 단어 경계 또는 완전 일치
                if (var_lower == text_lower or 
                    re.search(r'\b' + re.escape(var_lower) + r'\b', text_lower)):
                    return True
            else:
                # 긴 이름은 부분 매칭도 허용
                if (re.search(r'\b' + re.escape(var_lower) + r'\b', text_lower) or 
                    var_lower in text_lower):
                    return True
        
        return False
    
    def analyze_all_companies(self) -> List[Dict]:
        """모든 기업에 대해 뉴스 분석 실행"""
        # 상장기업 목록 로드
        self.load_companies()
        
        if self.companies_df is None or len(self.companies_df) == 0:
            logger.error("상장기업 목록을 불러올 수 없습니다.")
            return []
        
        results = []
        logger.info(f"이 {len(self.companies_df)}개 기업 분석 시작")
        
        # 처리 상태 추적
        processed_companies = 0
        companies_with_data = 0
        
        # 기업별 분석
        for idx, company_row in tqdm(self.companies_df.iterrows(), total=len(self.companies_df), desc="기업별 뉴스 수집"):
            company_name = company_row.get('종목명', '')
            
            if not company_name:
                continue
            
            processed_companies += 1
            logger.info(f"\n[{processed_companies}/{len(self.companies_df)}] {company_name} 분석 시작")
            
            # 기업 검색 변형 생성
            variations = self.generate_company_search_variations(company_row)
            
            if not variations:
                logger.warning(f"{company_name}: 검색 변형을 생성할 수 없음")
                continue
            
            logger.info(f"  검색 변형: {variations}")
            
            company_articles = []
            
            # 각 변형으로 검색
            for variation in variations:
                if len(company_articles) >= self.MAX_ARTICLES_PER_COMPANY:
                    break
                
                remaining_articles = self.MAX_ARTICLES_PER_COMPANY - len(company_articles)
                articles = self.search_news_for_query(
                    variation, 
                    self.START_DATE, 
                    self.END_DATE, 
                    min(200, remaining_articles)  # 변형당 최대 200개
                )
                
                company_articles.extend(articles)
                
                if articles:
                    logger.info(f"    [{variation}] {len(articles)}건 수집")
                else:
                    logger.info(f"    [{variation}] 0건")
            
            # 중복 제거
            unique_articles = []
            seen_links = set()
            for art in company_articles:
                if art['link'] not in seen_links:
                    unique_articles.append(art)
                    seen_links.add(art['link'])
            
            logger.info(f"  중복 제거 후: {len(unique_articles)}건")
            
            if not unique_articles:
                logger.info(f"  {company_name}: 수집된 기사 없음")
                continue
            
            # 각 기사 분석
            company_results = []
            data_asset_articles = []
            
            logger.info(f"  기사 분석 중...")
            for article in tqdm(unique_articles, desc=f"{company_name} 기사 분석", leave=False):
                # 전문 내용 가져오기
                if not article.get('full_content'):
                    article['full_content'] = self.get_full_article_content(article['link'])
                
                # 분석할 텍스트 (제목 + 요약 + 본문)
                full_text = f"{article['title']} {article['description']} {article.get('full_content', '')}"
                
                # 기업명 매칭 확인
                if not self.match_company(full_text, variations, company_row.to_dict()):
                    continue
                
                # 데이터 자산 키워드 매칭
                matched_categories = self.match_keywords_by_category(full_text)
                
                # 매칭된 키워드가 있는지 확인
                has_data_keywords = any(keywords for keywords in matched_categories.values())
                
                if has_data_keywords:
                    data_asset_articles.append({
                        'article': article,
                        'matched_categories': matched_categories,
                        'matched_keywords_count': sum(len(keywords) for keywords in matched_categories.values()),
                        'full_text_length': len(full_text)
                    })
            
            if data_asset_articles:
                companies_with_data += 1
                
                # 기업별 집계 정보 생성
                total_articles = len(data_asset_articles)
                
                # 카테고리별 기사 수 집계
                category_counts = {}
                all_matched_keywords = set()
                
                for item in data_asset_articles:
                    for category, keywords in item['matched_categories'].items():
                        if keywords:
                            category_counts[category] = category_counts.get(category, 0) + 1
                            all_matched_keywords.update(keywords)
                
                # 연도별 기사 분포
                yearly_distribution = {}
                for item in data_asset_articles:
                    year = item['article']['pubDate'][:4]
                    yearly_distribution[year] = yearly_distribution.get(year, 0) + 1
                
                # 기업 결과 저장
                company_result = {
                    '기업정보': {
                        '종목명': company_name,
                        '종목코드': company_row.get('종목코드', ''),
                        '시장구분': company_row.get('시장구분', ''),
                        '검색변형수': len(variations)
                    },
                    '데이터자산_기사_통계': {
                        '총_기사수': total_articles,
                        '매칭된_키워드수': len(all_matched_keywords),
                        '활용_카테고리수': len([c for c, count in category_counts.items() if count > 0])
                    },
                    '카테고리별_기사수': category_counts,
                    '연도별_분포': yearly_distribution,
                    '매칭된_키워드_목록': list(all_matched_keywords),
                    '대표_기사들': [
                        {
                            '날짜': item['article']['pubDate'],
                            '제목': item['article']['title'],
                            '링크': item['article']['link'],
                            '매칭_키워드수': item['matched_keywords_count'],
                            '매칭_카테고리': [cat for cat, kws in item['matched_categories'].items() if kws]
                        }
                        for item in sorted(data_asset_articles, key=lambda x: x['matched_keywords_count'], reverse=True)[:5]
                    ]
                }
                
                results.append(company_result)
                
                logger.info(f"  ✓ {company_name}: 데이터자산 관련 기사 {total_articles}건 발견")
                logger.info(f"    - 매칭 키워드: {len(all_matched_keywords)}개")
                logger.info(f"    - 활용 카테고리: {list(category_counts.keys())}")
            else:
                logger.info(f"  {company_name}: 데이터자산 관련 기사 없음")
        
        # 최종 결과 요약
        logger.info(f"\n=== 분석 완료 ===")
        logger.info(f"총 처리 기업수: {processed_companies}")
        logger.info(f"데이터자산 관련 기업수: {companies_with_data}")
        logger.info(f"전체 수집률: {companies_with_data/processed_companies*100:.1f}%")
        
        return results
    
    def save_results_to_excel(self, results: List[Dict], filename: str = None):
        """결과를 Excel 파일로 저장"""
        if not results:
            logger.warning("저장할 결과가 없습니다.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"데이터자산_뉴스분석_결과_{timestamp}.xlsx"
        
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # 1. 기업별 요약 시트
            summary_data = []
            for result in results:
                company_info = result['기업정보']
                stats = result['데이터자산_기사_통계']
                category_counts = result['카테고리별_기사수']
                yearly_dist = result['연도별_분포']
                
                summary_row = {
                    '종목명': company_info['종목명'],
                    '종목코드': company_info['종목코드'],
                    '시장구분': company_info['시장구분'],
                    '총_기사수': stats['총_기사수'],
                    '매칭_키워드수': stats['매칭된_키워드수'],
                    '활용_카테고리수': stats['활용_카테고리수'],
                    '기본_카테고리': category_counts.get('기본', 0),
                    '인프라_카테고리': category_counts.get('인프라', 0),
                    '활용분야_카테고리': category_counts.get('활용분야', 0),
                    '무형연계자산_카테고리': category_counts.get('무형연계자산', 0),
                    '신흥영역_카테고리': category_counts.get('신흥영역', 0),
                    '최근5년_기사수': sum(count for year, count in yearly_dist.items() if int(year) >= 2020),
                    '매칭_키워드_목록': ', '.join(result['매칭된_키워드_목록'][:10])  # 상위 10개만
                }
                summary_data.append(summary_row)
            
            summary_df = pd.DataFrame(summary_data)
            summary_df = summary_df.sort_values('총_기사수', ascending=False)
            summary_df.to_excel(writer, sheet_name='기업별_요약', index=False)
            
            # 2. 상세 기사 목록 시트
            detailed_data = []
            for result in results:
                company_name = result['기업정보']['종목명']
                for article_info in result['대표_기사들']:
                    detailed_row = {
                        '기업명': company_name,
                        '종목코드': result['기업정보']['종목코드'],
                        '시장구분': result['기업정보']['시장구분'],
                        '기사_날짜': article_info['날짜'],
                        '기사_제목': article_info['제목'],
                        '기사_링크': article_info['링크'],
                        '매칭_키워드수': article_info['매칭_키워드수'],
                        '매칭_카테고리': ', '.join(article_info['매칭_카테고리'])
                    }
                    detailed_data.append(detailed_row)
            
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df = detailed_df.sort_values(['매칭_키워드수', '기사_날짜'], ascending=[False, False])
            detailed_df.to_excel(writer, sheet_name='상세_기사목록', index=False)
            
            # 3. 카테고리별 통계 시트
            category_stats = {}
            for category in self.data_asset_keywords.keys():
                category_stats[category] = {
                    '관련_기업수': sum(1 for r in results if r['카테고리별_기사수'].get(category, 0) > 0),
                    '총_기사수': sum(r['카테고리별_기사수'].get(category, 0) for r in results),
                    '평균_기사수': sum(r['카테고리별_기사수'].get(category, 0) for r in results) / len(results),
                    '관련_키워드수': len(self.data_asset_keywords[category])
                }
            
            category_df = pd.DataFrame(category_stats).T.reset_index()
            category_df.columns = ['카테고리', '관련_기업수', '총_기사수', '평균_기사수', '관련_키워드수']
            category_df.to_excel(writer, sheet_name='카테고리별_통계', index=False)
            
            # 4. 연도별 트렌드 시트
            yearly_trends = {}
            for result in results:
                for year, count in result['연도별_분포'].items():
                    if year not in yearly_trends:
                        yearly_trends[year] = 0
                    yearly_trends[year] += count
            
            trend_df = pd.DataFrame(list(yearly_trends.items()), columns=['연도', '총_기사수'])
            trend_df = trend_df.sort_values('연도')
            trend_df.to_excel(writer, sheet_name='연도별_트렌드', index=False)
        
        logger.info(f"결과가 {filename}에 저장되었습니다.")
        return filename
    
    def generate_analysis_report(self, results: List[Dict]) -> str:
        """분석 결과 리포트 생성"""
        if not results:
            return "분석 결과가 없습니다."
        
        report = []
        report.append("=" * 60)
        report.append("데이터자산 관련 뉴스 분석 리포트")
        report.append("=" * 60)
        report.append("")
        
        # 전체 통계
        total_companies = len(results)
        total_articles = sum(r['데이터자산_기사_통계']['총_기사수'] for r in results)
        total_keywords = sum(r['데이터자산_기사_통계']['매칭된_키워드수'] for r in results)
        
        report.append(f"📊 전체 통계")
        report.append(f"  - 분석 대상 기업수: {total_companies}개")
        report.append(f"  - 수집된 데이터자산 관련 기사수: {total_articles}건")
        report.append(f"  - 매칭된 총 키워드 수: {total_keywords}개")
        report.append(f"  - 기업당 평균 기사수: {total_articles/total_companies:.1f}건")
        report.append("")
        
        # 상위 기업들
        top_companies = sorted(results, key=lambda x: x['데이터자산_기사_통계']['총_기사수'], reverse=True)[:10]
        report.append("🏆 데이터자산 관련 기사 상위 10개 기업")
        for i, company in enumerate(top_companies, 1):
            name = company['기업정보']['종목명']
            count = company['데이터자산_기사_통계']['총_기사수']
            keywords = company['데이터자산_기사_통계']['매칭된_키워드수']
            report.append(f"  {i:2d}. {name} - {count}건 (키워드 {keywords}개)")
        report.append("")
        
        # 카테고리별 분석
        category_analysis = {}
        for category in self.data_asset_keywords.keys():
            related_companies = [r for r in results if r['카테고리별_기사수'].get(category, 0) > 0]
            total_cat_articles = sum(r['카테고리별_기사수'].get(category, 0) for r in results)
            category_analysis[category] = {
                'companies': len(related_companies),
                'articles': total_cat_articles
            }
        
        report.append("📈 카테고리별 분석")
        for category, data in category_analysis.items():
            report.append(f"  - {category}: {data['companies']}개 기업, {data['articles']}건 기사")
        report.append("")
        
        # 연도별 트렌드
        yearly_totals = {}
        for result in results:
            for year, count in result['연도별_분포'].items():
                yearly_totals[year] = yearly_totals.get(year, 0) + count
        
        report.append("📅 연도별 기사 발행 트렌드")
        for year in sorted(yearly_totals.keys()):
            report.append(f"  - {year}년: {yearly_totals[year]}건")
        report.append("")
        
        # 주요 키워드 분석
        all_keywords = []
        for result in results:
            all_keywords.extend(result['매칭된_키워드_목록'])
        
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        top_keywords = keyword_counts.most_common(15)
        
        report.append("🔍 주요 매칭 키워드 (상위 15개)")
        for keyword, count in top_keywords:
            report.append(f"  - {keyword}: {count}회")
        report.append("")
        
        report.append("=" * 60)
        
        return "\n".join(report)

def main():
    """메인 실행 함수"""
    try:
        # 분석기 초기화
        analyzer = NewsAnalyzer()
        
        # 전체 기업 분석 실행
        logger.info("데이터자산 뉴스 분석을 시작합니다...")
        results = analyzer.analyze_all_companies()
        
        if not results:
            logger.warning("분석 결과가 없습니다.")
            return
        
        # Excel 파일로 저장
        excel_file = analyzer.save_results_to_excel(results)
        
        # 분석 리포트 생성 및 출력
        report = analyzer.generate_analysis_report(results)
        print(report)
        
        # 리포트를 텍스트 파일로도 저장
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"데이터자산_분석_리포트_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"분석이 완료되었습니다.")
        logger.info(f"Excel 결과: {excel_file}")
        logger.info(f"텍스트 리포트: {report_file}")
        
    except Exception as e:
        logger.error(f"실행 중 오류 발생: {e}")
        raise

if __name__ == "__main__":
    main()