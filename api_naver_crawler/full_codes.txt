import requests
import pandas as pd
import os
import re
import time
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
from typing import Dict, List, Set, Tuple, Optional
from bs4 import BeautifulSoup
from pykrx import stock
import config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KRXDataCollector:
    """PyKRXë¥¼ ì‚¬ìš©í•œ ìƒì¥ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        pass
        
    def get_all_listed_companies(self) -> pd.DataFrame:
        """ì „ì²´ ìƒì¥ê¸°ì—… ëª©ë¡ì„ ê°€ì ¸ì˜´ (PyKRX ì‚¬ìš©)"""
        logger.info("PyKRXë¥¼ í†µí•´ ì „ì²´ ìƒì¥ê¸°ì—… ëª©ë¡ ì¡°íšŒ ì¤‘...")
        
        try:
            # ìµœê·¼ ì˜ì—…ì¼ ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ìƒì¥ê¸°ì—… ì¡°íšŒ
            all_companies = []
            
            # KOSPI ìƒì¥ê¸°ì—…
            logger.info("KOSPI ìƒì¥ê¸°ì—… ì¡°íšŒ ì¤‘...")
            kospi_tickers = stock.get_market_ticker_list(market="KOSPI")
            for ticker in tqdm(kospi_tickers, desc="KOSPI ê¸°ì—… ì •ë³´ ìˆ˜ì§‘"):
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_companies.append({
                        'ì¢…ëª©ëª…': name,
                        'ì¢…ëª©ì½”ë“œ': ticker,
                        'ì‹œì¥êµ¬ë¶„': 'KOSPI'
                    })
                    time.sleep(0.01)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                except Exception as e:
                    logger.warning(f"KOSPI {ticker} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # KOSDAQ ìƒì¥ê¸°ì—…
            logger.info("KOSDAQ ìƒì¥ê¸°ì—… ì¡°íšŒ ì¤‘...")
            kosdaq_tickers = stock.get_market_ticker_list(market="KOSDAQ")
            for ticker in tqdm(kosdaq_tickers, desc="KOSDAQ ê¸°ì—… ì •ë³´ ìˆ˜ì§‘"):
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_companies.append({
                        'ì¢…ëª©ëª…': name,
                        'ì¢…ëª©ì½”ë“œ': ticker,
                        'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'
                    })
                    time.sleep(0.01)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                except Exception as e:
                    logger.warning(f"KOSDAQ {ticker} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # KONEX ìƒì¥ê¸°ì—… (ì¶”ê°€)
            logger.info("KONEX ìƒì¥ê¸°ì—… ì¡°íšŒ ì¤‘...")
            try:
                konex_tickers = stock.get_market_ticker_list(market="KONEX")
                for ticker in tqdm(konex_tickers, desc="KONEX ê¸°ì—… ì •ë³´ ìˆ˜ì§‘"):
                    try:
                        name = stock.get_market_ticker_name(ticker)
                        all_companies.append({
                            'ì¢…ëª©ëª…': name,
                            'ì¢…ëª©ì½”ë“œ': ticker,
                            'ì‹œì¥êµ¬ë¶„': 'KONEX'
                        })
                        time.sleep(0.01)
                    except Exception as e:
                        logger.warning(f"KONEX {ticker} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
            except Exception as e:
                logger.warning(f"KONEX ì¡°íšŒ ì˜¤ë¥˜: {e}")
            
            companies_df = pd.DataFrame(all_companies)
            logger.info(f"ì´ {len(companies_df)}ê°œ ìƒì¥ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
            return companies_df
            
        except Exception as e:
            logger.error(f"PyKRX ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_fallback_companies()
    
    def _get_fallback_companies(self) -> pd.DataFrame:
        """PyKRX ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì£¼ìš” ìƒì¥ê¸°ì—… ëª©ë¡ ë°˜í™˜"""
        logger.info("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì£¼ìš” ìƒì¥ê¸°ì—… ëª©ë¡ ìƒì„± ì¤‘...")
        
        # ì£¼ìš” ìƒì¥ê¸°ì—…ë“¤ (ì•½ 100ê°œ ì •ë„ í¬í•¨)
        major_companies = [
            {'ì¢…ëª©ëª…': 'ì‚¼ì„±ì „ì', 'ì¢…ëª©ì½”ë“œ': '005930', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'SKí•˜ì´ë‹‰ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '000660', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '207940', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'ì¢…ëª©ì½”ë“œ': '373220', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í˜„ëŒ€ìë™ì°¨', 'ì¢…ëª©ì½”ë“œ': '005380', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'KBê¸ˆìœµ', 'ì¢…ëª©ì½”ë“œ': '105560', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ê¸°ì•„', 'ì¢…ëª©ì½”ë“œ': '000270', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì…€íŠ¸ë¦¬ì˜¨', 'ì¢…ëª©ì½”ë“œ': '068270', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'NAVER', 'ì¢…ëª©ì½”ë“œ': '035420', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì¹´ì¹´ì˜¤', 'ì¢…ëª©ì½”ë“œ': '035720', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì‚¼ì„±ë¬¼ì‚°', 'ì¢…ëª©ì½”ë“œ': '028260', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '012330', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'POSCOí™€ë”©ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '005490', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í•œêµ­ì „ë ¥', 'ì¢…ëª©ì½”ë“œ': '015760', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'LGí™”í•™', 'ì¢…ëª©ì½”ë“œ': '051910', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í¬ë˜í”„í†¤', 'ì¢…ëª©ì½”ë“œ': '259960', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì¹´ì¹´ì˜¤ë±…í¬', 'ì¢…ëª©ì½”ë“œ': '323410', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì‚¼ì„±SDS', 'ì¢…ëª©ì½”ë“œ': '018260', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'KT', 'ì¢…ëª©ì½”ë“œ': '030200', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'LGì „ì', 'ì¢…ëª©ì½”ë“œ': '066570', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'SKí…”ë ˆì½¤', 'ì¢…ëª©ì½”ë“œ': '017670', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í•˜ì´ë¸Œ', 'ì¢…ëª©ì½”ë“œ': '352820', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ëŒ€í•œí•­ê³µ', 'ì¢…ëª©ì½”ë“œ': '003490', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì¹´ì¹´ì˜¤í˜ì´', 'ì¢…ëª©ì½”ë“œ': '377300', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì•„ëª¨ë ˆí¼ì‹œí”½', 'ì¢…ëª©ì½”ë“œ': '090430', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'NHíˆ¬ìì¦ê¶Œ', 'ì¢…ëª©ì½”ë“œ': '005940', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'LS Electric', 'ì¢…ëª©ì½”ë“œ': '010120', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì½”ì›¨ì´', 'ì¢…ëª©ì½”ë“œ': '021240', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'SKë°”ì´ì˜¤íŒœ', 'ì¢…ëª©ì½”ë“œ': '326030', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í•œí™”ì†”ë£¨ì…˜', 'ì¢…ëª©ì½”ë“œ': '009830', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'LGìœ í”ŒëŸ¬ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '032640', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í‚¤ì›€ì¦ê¶Œ', 'ì¢…ëª©ì½”ë“œ': '039490', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'SKì´ë…¸ë² ì´ì…˜', 'ì¢…ëª©ì½”ë“œ': '096770', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë‚ ', 'ì¢…ëª©ì½”ë“œ': '047050', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'LGë””ìŠ¤í”Œë ˆì´', 'ì¢…ëª©ì½”ë“œ': '034220', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í•œêµ­ì¡°ì„ í•´ì–‘', 'ì¢…ëª©ì½”ë“œ': '009540', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì‚¼ì„±í™”ì¬', 'ì¢…ëª©ì½”ë“œ': '000810', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'ì‹ í•œì§€ì£¼', 'ì¢…ëª©ì½”ë“œ': '055550', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'í•˜ë‚˜ê¸ˆìœµì§€ì£¼', 'ì¢…ëª©ì½”ë“œ': '086790', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            {'ì¢…ëª©ëª…': 'SK', 'ì¢…ëª©ì½”ë“œ': '034730', 'ì‹œì¥êµ¬ë¶„': 'KOSPI'},
            # KOSDAQ ì£¼ìš” ê¸°ì—…ë“¤
            {'ì¢…ëª©ëª…': 'HLB', 'ì¢…ëª©ì½”ë“œ': '028300', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ì—ì½”í”„ë¡œ', 'ì¢…ëª©ì½”ë“œ': '086520', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ì—ì½”í”„ë¡œë¹„ì— ', 'ì¢…ëª©ì½”ë“œ': '247540', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'í„ì–´ë¹„ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '263750', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ì—”ì”¨ì†Œí”„íŠ¸', 'ì¢…ëª©ì½”ë“œ': '036570', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ìœ„ë©”ì´ë“œ', 'ì¢…ëª©ì½”ë“œ': '112040', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ì»´íˆ¬ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '078340', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ë„¤ì˜¤ìœ„ì¦ˆ', 'ì¢…ëª©ì½”ë“œ': '095660', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ì•Œí…Œì˜¤ì  ', 'ì¢…ëª©ì½”ë“œ': '196170', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
            {'ì¢…ëª©ëª…': 'ë©”ë””í†¡ìŠ¤', 'ì¢…ëª©ì½”ë“œ': '086900', 'ì‹œì¥êµ¬ë¶„': 'KOSDAQ'},
        ]
        
        return pd.DataFrame(major_companies)

class NewsAnalyzer:
    def __init__(self):
        self.CLIENT_ID = config.client_id
        self.CLIENT_SECRET = config.client_secret
        self.BASE_URL = 'https://openapi.naver.com/v1/search/news.json'
        
        # KRX ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.krx_collector = KRXDataCollector()
        
        # ì„¤ì •ê°’
        self.START_DATE = '2015-01-01'
        self.END_DATE = '2024-12-31'
        self.MAX_ARTICLES_PER_COMPANY = 500  # ê¸°ì—…ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì¡°ì •
        self.MAX_VARIATIONS_PER_COMPANY = 3   # ê¸°ì—…ë‹¹ ê²€ìƒ‰ ë³€í˜• ìˆ˜ ì œí•œ
        self.REQUEST_DELAY = 0.1
        
        # ë°ì´í„° ìì‚° í‚¤ì›Œë“œë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
        self.data_asset_keywords = {
            'ê¸°ë³¸': [
                'ë°ì´í„°', 'ë°ì´í„°ìì‚°', 'ë¹…ë°ì´í„°', 'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹',
                'ë”¥ëŸ¬ë‹', 'ì•Œê³ ë¦¬ì¦˜', 'ë°ì´í„°ë¶„ì„', 'DB', 'ë°ì´í„°ë² ì´ìŠ¤'
            ],
            'ì¸í”„ë¼': [
                'í´ë¼ìš°ë“œ', 'ë°ì´í„°ì„¼í„°', 'ë°ì´í„° í”Œë«í¼', 'ë°ì´í„° ë ˆì´í¬',
                'ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤', 'ETL', 'API', 'ë¶„ì‚°ì €ì¥', 'ì—£ì§€ì»´í“¨íŒ…',
                'ìŠˆí¼ì»´í“¨í„°', 'GPU', 'NPU'
            ],
            'í™œìš©ë¶„ì•¼': [
                'ìì—°ì–¸ì–´ì²˜ë¦¬', 'ì»´í“¨í„°ë¹„ì „', 'ì¶”ì²œì‹œìŠ¤í…œ', 'ì˜ˆì¸¡ëª¨ë¸ë§',
                'í—¬ìŠ¤ ë°ì´í„°', 'ìœ ì „ì²´ ë°ì´í„°', 'ë°”ì´ì˜¤ë°ì´í„°',
                'IoT ë°ì´í„°', 'ììœ¨ì£¼í–‰ ë°ì´í„°',
                'CRM ë°ì´í„°', 'ERP ë°ì´í„°', 'ë¡œê·¸ë°ì´í„°', 'ì‚¬ìš©ì í–‰ë™ ë°ì´í„°'
            ],
            'ë¬´í˜•ì—°ê³„ìì‚°': [
                'ì†Œí”„íŠ¸ì›¨ì–´ ìì‚°', 'ì§€ì‹ì¬ì‚°ê¶Œ', 'ë””ì§€í„¸ íŠ¸ìœˆ', 'ë©”íƒ€ë²„ìŠ¤ ìì‚°'
            ],
            'ì‹ í¥ì˜ì—­': [
                'ìƒì„±í˜•AI', 'LLM', 'í•©ì„±ë°ì´í„°', 'ë°ì´í„° ë¼ë²¨ë§',
                'ë°ì´í„° ê±°ë²„ë„ŒìŠ¤', 'ë°ì´í„° í’ˆì§ˆ', 'ë°ì´í„° ìœ¤ë¦¬', 'ë°ì´í„° ë³´ì•ˆ', 'ë°ì´í„° í”„ë¼ì´ë²„ì‹œ'
            ]
        }
        
        # ì „ì²´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        self.all_keywords = []
        for keywords in self.data_asset_keywords.values():
            self.all_keywords.extend(keywords)
        
        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì¤‘ë³µ ë°©ì§€
        self.collected_articles: Set[str] = set()
        
        # ìƒì¥ê¸°ì—… ì •ë³´ ì €ì¥
        self.companies_df: Optional[pd.DataFrame] = None
        
    def load_companies(self):
        """ìƒì¥ê¸°ì—… ëª©ë¡ ë¡œë“œ"""
        if self.companies_df is None:
            self.companies_df = self.krx_collector.get_all_listed_companies()
            
    def generate_company_search_variations(self, company_row) -> List[str]:
        """ê¸°ì—… ê²€ìƒ‰ ë³€í˜• ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        variations = set()
        
        # ê¸°ë³¸ ì •ë³´ì—ì„œ ë³€í˜• ìƒì„±
        company_name = company_row.get('ì¢…ëª©ëª…', '')
        stock_code = company_row.get('ì¢…ëª©ì½”ë“œ', '')
        
        if not company_name:
            return []
        
        # ê¸°ë³¸ ì´ë¦„ ì¶”ê°€
        variations.add(company_name)
        
        # ì£¼ì‹íšŒì‚¬, (ì£¼) ì œê±°/ì¶”ê°€ ë³€í˜•
        clean_name = re.sub(r'ì£¼ì‹íšŒì‚¬$|ãˆœ|ì£¼ì‹íšŒì‚¬|ï¼ˆì£¼ï¼‰|\(ì£¼\)', '', company_name).strip()
        if clean_name and len(clean_name) >= 2:
            variations.add(clean_name)
            variations.add(clean_name + 'ì£¼ì‹íšŒì‚¬')
            variations.add(f'(ì£¼){clean_name}')
            variations.add(f'ãˆœ{clean_name}')
        
        # ê³µë°± ì²˜ë¦¬
        if ' ' in company_name:
            variations.add(company_name.replace(' ', ''))
        if '-' in company_name:
            variations.add(company_name.replace('-', ' '))
        
        # ì˜ë¬¸ ëŒ€ì†Œë¬¸ì ë³€í˜• (ì˜ë¬¸ í¬í•¨ì‹œì—ë§Œ)
        if re.search(r'[a-zA-Z]', company_name):
            variations.add(company_name.upper())
            variations.add(company_name.lower())
            variations.add(company_name.title())
        
        # ì¢…ëª©ì½”ë“œ ì¶”ê°€ (6ìë¦¬ì¸ ê²½ìš°ë§Œ)
        if stock_code and len(stock_code) == 6:
            variations.add(stock_code)
        
        # ë„ˆë¬´ ì§§ì€ ë³€í˜•ë“¤ ì œê±° (2ê¸€ì ë¯¸ë§Œ)
        variations = {v for v in variations if len(v) >= 2}
        
        # ìƒìœ„ Nê°œë§Œ ë°˜í™˜ (API í˜¸ì¶œ ìµœì í™”)
        return list(variations)[:self.MAX_VARIATIONS_PER_COMPANY]
    
    def get_full_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ì „ë¬¸ ê°€ì ¸ì˜¤ê¸° (ì•ˆì •ì ì¸ ë²„ì „)"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            time.sleep(0.2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì„ íƒìë“¤
                content_selectors = [
                    '#dic_area',           # ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸
                    '.news_end',           # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
                    '.article_body',       # ê¸°ì‚¬ ë³¸ë¬¸
                    '.article-body',
                    '.content',
                    '.article_content',
                    'article',
                    '.view_txt',           # ì¼ë¶€ ë‰´ìŠ¤ì‚¬
                    '.article-wrap',       # ì¶”ê°€ ì„ íƒì
                    '.news_view'           # ì¶”ê°€ ì„ íƒì
                ]
                
                article_content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        # ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸ ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in content_elem.select('script, style, .ad, .advertisement, .social, .share, .comment, .footer'):
                            unwanted.decompose()
                        
                        article_content = content_elem.get_text(strip=True)
                        if len(article_content) > 200:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë³¸ë¬¸ì¸ ê²½ìš°
                            break
                
                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                if not article_content or len(article_content) < 200:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±° í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    for unwanted in soup.select('script, style, nav, header, footer, .ad, .advertisement'):
                        unwanted.decompose()
                    article_content = soup.get_text()
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                article_content = re.sub(r'\s+', ' ', article_content).strip()
                return article_content[:5000]  # ìµœëŒ€ 5000ì
                
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
        
        return ""
    
    def search_news_for_query(self, query: str, start_date: str, end_date: str, max_articles: int = 500) -> List[Dict]:
        """íŠ¹ì • ì¿¼ë¦¬ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì•ˆì •í™”ëœ ë²„ì „)"""
        headers = {
            "X-Naver-Client-Id": self.CLIENT_ID,
            "X-Naver-Client-Secret": self.CLIENT_SECRET
        }
        
        articles = []
        display = 100
        
        for start in range(1, max_articles + 1, display):
            if start > 1000:  # ë„¤ì´ë²„ API ì œí•œ
                break
            
            current_display = min(display, 1000 - start + 1, max_articles - len(articles))
            if current_display <= 0:
                break
            
            params = {
                'query': query,
                'display': current_display,
                'start': start,
                'sort': 'sim'
            }
            
            try:
                time.sleep(self.REQUEST_DELAY)
                response = requests.get(self.BASE_URL, headers=headers, params=params, timeout=10)
                
                if response.status_code != 200:
                    logger.warning(f"[{query}] API ì˜¤ë¥˜ {response.status_code}")
                    if response.status_code == 429:  # Too Many Requests
                        time.sleep(1)  # ì¶”ê°€ ëŒ€ê¸°
                        continue
                    break
                
                data = response.json()
                items = data.get('items', [])
                
                if not items:
                    break
                
                for item in items:
                    try:
                        # ë‚ ì§œ íŒŒì‹±
                        pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')
                        pub_str = pub_date.strftime('%Y-%m-%d')
                        
                        if not (start_date <= pub_str <= end_date):
                            continue
                        
                        # ì¤‘ë³µ ì²´í¬
                        if item['link'] in self.collected_articles:
                            continue
                        
                        article = {
                            'pubDate': pub_str,
                            'title': self._clean_html_tags(item['title']),
                            'description': self._clean_html_tags(item['description']),
                            'link': item['link'],
                            'original_query': query,
                            'full_content': ''  # ë‚˜ì¤‘ì— ì±„ì›€
                        }
                        
                        articles.append(article)
                        self.collected_articles.add(item['link'])
                        
                        if len(articles) >= max_articles:
                            return articles
                            
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
                
                if len(items) < current_display:
                    break
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
                time.sleep(2)  # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì‹œ ëŒ€ê¸°
                break
            except Exception as e:
                logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                break
        
        return articles
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (ê°œì„ ëœ ë²„ì „)"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        clean = re.sub('<.*?>', '', text)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        html_entities = {
            '&lt;': '<', '&gt;': '>', '&amp;': '&', '&quot;': '"',
            '&#39;': "'", '&nbsp;': ' ', '&apos;': "'", 
            '&ldquo;': '"', '&rdquo;': '"', '&lsquo;': "'", '&rsquo;': "'",
            '&hellip;': '...', '&mdash;': 'â€”', '&ndash;': 'â€“'
        }
        
        for entity, replacement in html_entities.items():
            clean = clean.replace(entity, replacement)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        clean = re.sub(r'\s+', ' ', clean)
        
        return clean.strip()
    
    def match_keywords_by_category(self, text: str) -> Dict[str, List[str]]:
        """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ (ì •í™•ë„ ê°œì„ )"""
        text_lower = text.lower()
        matched_by_category = {}
        
        for category, keywords in self.data_asset_keywords.items():
            matched_keywords = []
            for keyword in keywords:
                keyword_lower = keyword.lower()
                
                # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë§¤ì¹­ ë˜ëŠ” ë¶€ë¶„ ë§¤ì¹­
                if (re.search(r'\b' + re.escape(keyword_lower) + r'\b', text_lower) or 
                    (len(keyword_lower) >= 3 and keyword_lower in text_lower)):
                    matched_keywords.append(keyword)
            
            matched_by_category[category] = matched_keywords
        
        return matched_by_category
    
    def match_company(self, text: str, company_variations: List[str], company_info: Dict) -> bool:
        """ê¸°ì—…ëª… ë§¤ì¹­ í™•ì¸ (ì •í™•ë„ ê°œì„ )"""
        text_lower = text.lower()
        
        for variation in company_variations:
            var_lower = variation.lower()
            
            # ê¸¸ì´ë³„ ë§¤ì¹­ ì „ëµ
            if len(var_lower) <= 2:
                # ì§§ì€ ì´ë¦„ì€ ì™„ì „ ì¼ì¹˜ë§Œ
                if var_lower == text_lower:
                    return True
            elif len(var_lower) <= 4:
                # ì¤‘ê°„ ê¸¸ì´ëŠ” ë‹¨ì–´ ê²½ê³„ ë˜ëŠ” ì™„ì „ ì¼ì¹˜
                if (var_lower == text_lower or 
                    re.search(r'\b' + re.escape(var_lower) + r'\b', text_lower)):
                    return True
            else:
                # ê¸´ ì´ë¦„ì€ ë¶€ë¶„ ë§¤ì¹­ë„ í—ˆìš©
                if (re.search(r'\b' + re.escape(var_lower) + r'\b', text_lower) or 
                    var_lower in text_lower):
                    return True
        
        return False
    
    def analyze_all_companies(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì—…ì— ëŒ€í•´ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰"""
        # ìƒì¥ê¸°ì—… ëª©ë¡ ë¡œë“œ
        self.load_companies()
        
        if self.companies_df is None or len(self.companies_df) == 0:
            logger.error("ìƒì¥ê¸°ì—… ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        results = []
        logger.info(f"ì´ {len(self.companies_df)}ê°œ ê¸°ì—… ë¶„ì„ ì‹œì‘")
        
        # ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
        processed_companies = 0
        companies_with_data = 0
        
        # ê¸°ì—…ë³„ ë¶„ì„
        for idx, company_row in tqdm(self.companies_df.iterrows(), total=len(self.companies_df), desc="ê¸°ì—…ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘"):
            company_name = company_row.get('ì¢…ëª©ëª…', '')
            
            if not company_name:
                continue
            
            processed_companies += 1
            logger.info(f"\n[{processed_companies}/{len(self.companies_df)}] {company_name} ë¶„ì„ ì‹œì‘")
            
            # ê¸°ì—… ê²€ìƒ‰ ë³€í˜• ìƒì„±
            variations = self.generate_company_search_variations(company_row)
            
            if not variations:
                logger.warning(f"{company_name}: ê²€ìƒ‰ ë³€í˜•ì„ ìƒì„±í•  ìˆ˜ ì—†ìŒ")
                continue
            
            logger.info(f"  ê²€ìƒ‰ ë³€í˜•: {variations}")
            
            company_articles = []
            
            # ê° ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰
            for variation in variations:
                if len(company_articles) >= self.MAX_ARTICLES_PER_COMPANY:
                    break
                
                remaining_articles = self.MAX_ARTICLES_PER_COMPANY - len(company_articles)
                articles = self.search_news_for_query(
                    variation, 
                    self.START_DATE, 
                    self.END_DATE, 
                    min(200, remaining_articles)  # ë³€í˜•ë‹¹ ìµœëŒ€ 200ê°œ
                )
                
                company_articles.extend(articles)
                
                if articles:
                    logger.info(f"    [{variation}] {len(articles)}ê±´ ìˆ˜ì§‘")
                else:
                    logger.info(f"    [{variation}] 0ê±´")
            
            # ì¤‘ë³µ ì œê±°
            unique_articles = []
            seen_links = set()
            for art in company_articles:
                if art['link'] not in seen_links:
                    unique_articles.append(art)
                    seen_links.add(art['link'])
            
            logger.info(f"  ì¤‘ë³µ ì œê±° í›„: {len(unique_articles)}ê±´")
            
            if not unique_articles:
                logger.info(f"  {company_name}: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŒ")
                continue
            
            # ê° ê¸°ì‚¬ ë¶„ì„
            company_results = []
            data_asset_articles = []
            
            logger.info(f"  ê¸°ì‚¬ ë¶„ì„ ì¤‘...")
            for article in tqdm(unique_articles, desc=f"{company_name} ê¸°ì‚¬ ë¶„ì„", leave=False):
                # ì „ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                if not article.get('full_content'):
                    article['full_content'] = self.get_full_article_content(article['link'])
                
                # ë¶„ì„í•  í…ìŠ¤íŠ¸ (ì œëª© + ìš”ì•½ + ë³¸ë¬¸)
                full_text = f"{article['title']} {article['description']} {article.get('full_content', '')}"
                
                # ê¸°ì—…ëª… ë§¤ì¹­ í™•ì¸
                if not self.match_company(full_text, variations, company_row.to_dict()):
                    continue
                
                # ë°ì´í„° ìì‚° í‚¤ì›Œë“œ ë§¤ì¹­
                matched_categories = self.match_keywords_by_category(full_text)
                
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                has_data_keywords = any(keywords for keywords in matched_categories.values())
                
                if has_data_keywords:
                    data_asset_articles.append({
                        'article': article,
                        'matched_categories': matched_categories,
                        'matched_keywords_count': sum(len(keywords) for keywords in matched_categories.values()),
                        'full_text_length': len(full_text)
                    })
            
            if data_asset_articles:
                companies_with_data += 1
                
                # ê¸°ì—…ë³„ ì§‘ê³„ ì •ë³´ ìƒì„±
                total_articles = len(data_asset_articles)
                
                # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„
                category_counts = {}
                all_matched_keywords = set()
                
                for item in data_asset_articles:
                    for category, keywords in item['matched_categories'].items():
                        if keywords:
                            category_counts[category] = category_counts.get(category, 0) + 1
                            all_matched_keywords.update(keywords)
                
                # ì—°ë„ë³„ ê¸°ì‚¬ ë¶„í¬
                yearly_distribution = {}
                for item in data_asset_articles:
                    year = item['article']['pubDate'][:4]
                    yearly_distribution[year] = yearly_distribution.get(year, 0) + 1
                
                # ê¸°ì—… ê²°ê³¼ ì €ì¥
                company_result = {
                    'ê¸°ì—…ì •ë³´': {
                        'ì¢…ëª©ëª…': company_name,
                        'ì¢…ëª©ì½”ë“œ': company_row.get('ì¢…ëª©ì½”ë“œ', ''),
                        'ì‹œì¥êµ¬ë¶„': company_row.get('ì‹œì¥êµ¬ë¶„', ''),
                        'ê²€ìƒ‰ë³€í˜•ìˆ˜': len(variations)
                    },
                    'ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„': {
                        'ì´_ê¸°ì‚¬ìˆ˜': total_articles,
                        'ë§¤ì¹­ëœ_í‚¤ì›Œë“œìˆ˜': len(all_matched_keywords),
                        'í™œìš©_ì¹´í…Œê³ ë¦¬ìˆ˜': len([c for c, count in category_counts.items() if count > 0])
                    },
                    'ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜': category_counts,
                    'ì—°ë„ë³„_ë¶„í¬': yearly_distribution,
                    'ë§¤ì¹­ëœ_í‚¤ì›Œë“œ_ëª©ë¡': list(all_matched_keywords),
                    'ëŒ€í‘œ_ê¸°ì‚¬ë“¤': [
                        {
                            'ë‚ ì§œ': item['article']['pubDate'],
                            'ì œëª©': item['article']['title'],
                            'ë§í¬': item['article']['link'],
                            'ë§¤ì¹­_í‚¤ì›Œë“œìˆ˜': item['matched_keywords_count'],
                            'ë§¤ì¹­_ì¹´í…Œê³ ë¦¬': [cat for cat, kws in item['matched_categories'].items() if kws]
                        }
                        for item in sorted(data_asset_articles, key=lambda x: x['matched_keywords_count'], reverse=True)[:5]
                    ]
                }
                
                results.append(company_result)
                
                logger.info(f"  âœ“ {company_name}: ë°ì´í„°ìì‚° ê´€ë ¨ ê¸°ì‚¬ {total_articles}ê±´ ë°œê²¬")
                logger.info(f"    - ë§¤ì¹­ í‚¤ì›Œë“œ: {len(all_matched_keywords)}ê°œ")
                logger.info(f"    - í™œìš© ì¹´í…Œê³ ë¦¬: {list(category_counts.keys())}")
            else:
                logger.info(f"  {company_name}: ë°ì´í„°ìì‚° ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        logger.info(f"\n=== ë¶„ì„ ì™„ë£Œ ===")
        logger.info(f"ì´ ì²˜ë¦¬ ê¸°ì—…ìˆ˜: {processed_companies}")
        logger.info(f"ë°ì´í„°ìì‚° ê´€ë ¨ ê¸°ì—…ìˆ˜: {companies_with_data}")
        logger.info(f"ì „ì²´ ìˆ˜ì§‘ë¥ : {companies_with_data/processed_companies*100:.1f}%")
        
        return results
    
    def save_results_to_excel(self, results: List[Dict], filename: str = None):
        """ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥"""
        if not results:
            logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ë°ì´í„°ìì‚°_ë‰´ìŠ¤ë¶„ì„_ê²°ê³¼_{timestamp}.xlsx"
        
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # 1. ê¸°ì—…ë³„ ìš”ì•½ ì‹œíŠ¸
            summary_data = []
            for result in results:
                company_info = result['ê¸°ì—…ì •ë³´']
                stats = result['ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„']
                category_counts = result['ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜']
                yearly_dist = result['ì—°ë„ë³„_ë¶„í¬']
                
                summary_row = {
                    'ì¢…ëª©ëª…': company_info['ì¢…ëª©ëª…'],
                    'ì¢…ëª©ì½”ë“œ': company_info['ì¢…ëª©ì½”ë“œ'],
                    'ì‹œì¥êµ¬ë¶„': company_info['ì‹œì¥êµ¬ë¶„'],
                    'ì´_ê¸°ì‚¬ìˆ˜': stats['ì´_ê¸°ì‚¬ìˆ˜'],
                    'ë§¤ì¹­_í‚¤ì›Œë“œìˆ˜': stats['ë§¤ì¹­ëœ_í‚¤ì›Œë“œìˆ˜'],
                    'í™œìš©_ì¹´í…Œê³ ë¦¬ìˆ˜': stats['í™œìš©_ì¹´í…Œê³ ë¦¬ìˆ˜'],
                    'ê¸°ë³¸_ì¹´í…Œê³ ë¦¬': category_counts.get('ê¸°ë³¸', 0),
                    'ì¸í”„ë¼_ì¹´í…Œê³ ë¦¬': category_counts.get('ì¸í”„ë¼', 0),
                    'í™œìš©ë¶„ì•¼_ì¹´í…Œê³ ë¦¬': category_counts.get('í™œìš©ë¶„ì•¼', 0),
                    'ë¬´í˜•ì—°ê³„ìì‚°_ì¹´í…Œê³ ë¦¬': category_counts.get('ë¬´í˜•ì—°ê³„ìì‚°', 0),
                    'ì‹ í¥ì˜ì—­_ì¹´í…Œê³ ë¦¬': category_counts.get('ì‹ í¥ì˜ì—­', 0),
                    'ìµœê·¼5ë…„_ê¸°ì‚¬ìˆ˜': sum(count for year, count in yearly_dist.items() if int(year) >= 2020),
                    'ë§¤ì¹­_í‚¤ì›Œë“œ_ëª©ë¡': ', '.join(result['ë§¤ì¹­ëœ_í‚¤ì›Œë“œ_ëª©ë¡'][:10])  # ìƒìœ„ 10ê°œë§Œ
                }
                summary_data.append(summary_row)
            
            summary_df = pd.DataFrame(summary_data)
            summary_df = summary_df.sort_values('ì´_ê¸°ì‚¬ìˆ˜', ascending=False)
            summary_df.to_excel(writer, sheet_name='ê¸°ì—…ë³„_ìš”ì•½', index=False)
            
            # 2. ìƒì„¸ ê¸°ì‚¬ ëª©ë¡ ì‹œíŠ¸
            detailed_data = []
            for result in results:
                company_name = result['ê¸°ì—…ì •ë³´']['ì¢…ëª©ëª…']
                for article_info in result['ëŒ€í‘œ_ê¸°ì‚¬ë“¤']:
                    detailed_row = {
                        'ê¸°ì—…ëª…': company_name,
                        'ì¢…ëª©ì½”ë“œ': result['ê¸°ì—…ì •ë³´']['ì¢…ëª©ì½”ë“œ'],
                        'ì‹œì¥êµ¬ë¶„': result['ê¸°ì—…ì •ë³´']['ì‹œì¥êµ¬ë¶„'],
                        'ê¸°ì‚¬_ë‚ ì§œ': article_info['ë‚ ì§œ'],
                        'ê¸°ì‚¬_ì œëª©': article_info['ì œëª©'],
                        'ê¸°ì‚¬_ë§í¬': article_info['ë§í¬'],
                        'ë§¤ì¹­_í‚¤ì›Œë“œìˆ˜': article_info['ë§¤ì¹­_í‚¤ì›Œë“œìˆ˜'],
                        'ë§¤ì¹­_ì¹´í…Œê³ ë¦¬': ', '.join(article_info['ë§¤ì¹­_ì¹´í…Œê³ ë¦¬'])
                    }
                    detailed_data.append(detailed_row)
            
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df = detailed_df.sort_values(['ë§¤ì¹­_í‚¤ì›Œë“œìˆ˜', 'ê¸°ì‚¬_ë‚ ì§œ'], ascending=[False, False])
            detailed_df.to_excel(writer, sheet_name='ìƒì„¸_ê¸°ì‚¬ëª©ë¡', index=False)
            
            # 3. ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì‹œíŠ¸
            category_stats = {}
            for category in self.data_asset_keywords.keys():
                category_stats[category] = {
                    'ê´€ë ¨_ê¸°ì—…ìˆ˜': sum(1 for r in results if r['ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜'].get(category, 0) > 0),
                    'ì´_ê¸°ì‚¬ìˆ˜': sum(r['ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜'].get(category, 0) for r in results),
                    'í‰ê· _ê¸°ì‚¬ìˆ˜': sum(r['ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜'].get(category, 0) for r in results) / len(results),
                    'ê´€ë ¨_í‚¤ì›Œë“œìˆ˜': len(self.data_asset_keywords[category])
                }
            
            category_df = pd.DataFrame(category_stats).T.reset_index()
            category_df.columns = ['ì¹´í…Œê³ ë¦¬', 'ê´€ë ¨_ê¸°ì—…ìˆ˜', 'ì´_ê¸°ì‚¬ìˆ˜', 'í‰ê· _ê¸°ì‚¬ìˆ˜', 'ê´€ë ¨_í‚¤ì›Œë“œìˆ˜']
            category_df.to_excel(writer, sheet_name='ì¹´í…Œê³ ë¦¬ë³„_í†µê³„', index=False)
            
            # 4. ì—°ë„ë³„ íŠ¸ë Œë“œ ì‹œíŠ¸
            yearly_trends = {}
            for result in results:
                for year, count in result['ì—°ë„ë³„_ë¶„í¬'].items():
                    if year not in yearly_trends:
                        yearly_trends[year] = 0
                    yearly_trends[year] += count
            
            trend_df = pd.DataFrame(list(yearly_trends.items()), columns=['ì—°ë„', 'ì´_ê¸°ì‚¬ìˆ˜'])
            trend_df = trend_df.sort_values('ì—°ë„')
            trend_df.to_excel(writer, sheet_name='ì—°ë„ë³„_íŠ¸ë Œë“œ', index=False)
        
        logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    
    def generate_analysis_report(self, results: List[Dict]) -> str:
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not results:
            return "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = []
        report.append("=" * 60)
        report.append("ë°ì´í„°ìì‚° ê´€ë ¨ ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append("")
        
        # ì „ì²´ í†µê³„
        total_companies = len(results)
        total_articles = sum(r['ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„']['ì´_ê¸°ì‚¬ìˆ˜'] for r in results)
        total_keywords = sum(r['ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„']['ë§¤ì¹­ëœ_í‚¤ì›Œë“œìˆ˜'] for r in results)
        
        report.append(f"ğŸ“Š ì „ì²´ í†µê³„")
        report.append(f"  - ë¶„ì„ ëŒ€ìƒ ê¸°ì—…ìˆ˜: {total_companies}ê°œ")
        report.append(f"  - ìˆ˜ì§‘ëœ ë°ì´í„°ìì‚° ê´€ë ¨ ê¸°ì‚¬ìˆ˜: {total_articles}ê±´")
        report.append(f"  - ë§¤ì¹­ëœ ì´ í‚¤ì›Œë“œ ìˆ˜: {total_keywords}ê°œ")
        report.append(f"  - ê¸°ì—…ë‹¹ í‰ê·  ê¸°ì‚¬ìˆ˜: {total_articles/total_companies:.1f}ê±´")
        report.append("")
        
        # ìƒìœ„ ê¸°ì—…ë“¤
        top_companies = sorted(results, key=lambda x: x['ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„']['ì´_ê¸°ì‚¬ìˆ˜'], reverse=True)[:10]
        report.append("ğŸ† ë°ì´í„°ìì‚° ê´€ë ¨ ê¸°ì‚¬ ìƒìœ„ 10ê°œ ê¸°ì—…")
        for i, company in enumerate(top_companies, 1):
            name = company['ê¸°ì—…ì •ë³´']['ì¢…ëª©ëª…']
            count = company['ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„']['ì´_ê¸°ì‚¬ìˆ˜']
            keywords = company['ë°ì´í„°ìì‚°_ê¸°ì‚¬_í†µê³„']['ë§¤ì¹­ëœ_í‚¤ì›Œë“œìˆ˜']
            report.append(f"  {i:2d}. {name} - {count}ê±´ (í‚¤ì›Œë“œ {keywords}ê°œ)")
        report.append("")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_analysis = {}
        for category in self.data_asset_keywords.keys():
            related_companies = [r for r in results if r['ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜'].get(category, 0) > 0]
            total_cat_articles = sum(r['ì¹´í…Œê³ ë¦¬ë³„_ê¸°ì‚¬ìˆ˜'].get(category, 0) for r in results)
            category_analysis[category] = {
                'companies': len(related_companies),
                'articles': total_cat_articles
            }
        
        report.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„")
        for category, data in category_analysis.items():
            report.append(f"  - {category}: {data['companies']}ê°œ ê¸°ì—…, {data['articles']}ê±´ ê¸°ì‚¬")
        report.append("")
        
        # ì—°ë„ë³„ íŠ¸ë Œë“œ
        yearly_totals = {}
        for result in results:
            for year, count in result['ì—°ë„ë³„_ë¶„í¬'].items():
                yearly_totals[year] = yearly_totals.get(year, 0) + count
        
        report.append("ğŸ“… ì—°ë„ë³„ ê¸°ì‚¬ ë°œí–‰ íŠ¸ë Œë“œ")
        for year in sorted(yearly_totals.keys()):
            report.append(f"  - {year}ë…„: {yearly_totals[year]}ê±´")
        report.append("")
        
        # ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„
        all_keywords = []
        for result in results:
            all_keywords.extend(result['ë§¤ì¹­ëœ_í‚¤ì›Œë“œ_ëª©ë¡'])
        
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        top_keywords = keyword_counts.most_common(15)
        
        report.append("ğŸ” ì£¼ìš” ë§¤ì¹­ í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ)")
        for keyword, count in top_keywords:
            report.append(f"  - {keyword}: {count}íšŒ")
        report.append("")
        
        report.append("=" * 60)
        
        return "\n".join(report)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = NewsAnalyzer()
        
        # ì „ì²´ ê¸°ì—… ë¶„ì„ ì‹¤í–‰
        logger.info("ë°ì´í„°ìì‚° ë‰´ìŠ¤ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        results = analyzer.analyze_all_companies()
        
        if not results:
            logger.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Excel íŒŒì¼ë¡œ ì €ì¥
        excel_file = analyzer.save_results_to_excel(results)
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
        report = analyzer.generate_analysis_report(results)
        print(report)
        
        # ë¦¬í¬íŠ¸ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œë„ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"ë°ì´í„°ìì‚°_ë¶„ì„_ë¦¬í¬íŠ¸_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info(f"Excel ê²°ê³¼: {excel_file}")
        logger.info(f"í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸: {report_file}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()